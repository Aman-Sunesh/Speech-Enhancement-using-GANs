{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Enhancement using Conditional GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NYUAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System & Utilities\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Audio I/O & Processing\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# PyTorch Model Building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "# Demo / Deployment\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the mel spectrogram transform\n",
    "mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate = 16000,\n",
    "                                                         n_fft = 512, \n",
    "                                                         hop_length = 128, \n",
    "                                                         n_mels = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minus_one_to_one(data):\n",
    "    x_min = data.min()\n",
    "    x_max = data.max()\n",
    "    normalized_data = 2 * ((data - x_min) / (x_max - x_min)) - 1\n",
    "    return normalized_data, x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size, bucket_size = 1000, shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        sorted_indices = []\n",
    "        buckets = []\n",
    "\n",
    "        # Build list of (length, index) pairs\n",
    "        for i in range(len(lengths)):\n",
    "            sorted_indices.append((lengths[i], i))\n",
    "\n",
    "        # Sort by length\n",
    "        sorted_indices.sort()\n",
    "\n",
    "        # Keep only the indices in order\n",
    "        for j in range(len(sorted_indices)):\n",
    "            sorted_indices[j] = sorted_indices[j][1]\n",
    "\n",
    "        # Break into buckets of size 'bucket_size'\n",
    "        for i in range(0, len(sorted_indices), bucket_size):\n",
    "            bucket = sorted_indices[i : i + bucket_size]\n",
    "            buckets.append(bucket)\n",
    "\n",
    "        self.buckets = buckets\n",
    "\n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            if self.shuffle:\n",
    "                random.shuffle(bucket)\n",
    "\n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                yield bucket[i : i + self.batch_size]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        sum_batches = 0\n",
    "\n",
    "        for b in self.buckets:\n",
    "            sum_batches += (len(b) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        return sum_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch, T_max=512):\n",
    "    noisys, cleans, stats, names = [], [], [], []\n",
    "\n",
    "    for noisy, clean, st, name in batch:\n",
    "        # Truncate\n",
    "        noisy = noisy[..., :T_max]\n",
    "        clean = clean[..., :T_max]\n",
    "\n",
    "        # Pad if shorter\n",
    "        pad_no = T_max - noisy.shape[-1]\n",
    "        pad_cl = T_max - clean.shape[-1]\n",
    "\n",
    "        if pad_no>0:\n",
    "            noisy = F.pad(noisy, (0, pad_no))\n",
    "            clean = F.pad(clean, (0, pad_cl))\n",
    "\n",
    "        noisys.append(noisy)\n",
    "        cleans.append(clean)\n",
    "\n",
    "        stats.append(st)\n",
    "        names.append(name)\n",
    "\n",
    "    return torch.stack(noisys), torch.stack(cleans), stats, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root, mode = 'train'):\n",
    "        root = Path(root)\n",
    "        self.clean_dir = root / 'clean_trainset_wav'\n",
    "        self.noisy_dir = root / 'noisy_trainset_wav'\n",
    "\n",
    "        # Ensure directories exist\n",
    "        assert self.clean_dir.exists(), f\"{self.clean_dir} not found.\"\n",
    "        assert self.noisy_dir.exists(), f\"{self.noisy_dir} not found.\"\n",
    "\n",
    "        self.file_list = sorted(p.stem for p in self.clean_dir.glob(\"*.wav\"))\n",
    "\n",
    "        # Precompute mel-frame lengths for bucketing\n",
    "        self.lengths = []\n",
    "        for stem in self.file_list:\n",
    "            wav, sr = torchaudio.load(self.clean_dir / f\"{stem}.wav\")\n",
    "            if sr != 16000:\n",
    "                wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "            # compute mel (without log or norm)\n",
    "            mel = mel_spec_transform(wav)  # shape [1, 80, T]\n",
    "            self.lengths.append(mel.shape[-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.file_list[idx]\n",
    "\n",
    "        clean_path = self.clean_dir / f\"{basename}.wav\"\n",
    "        noisy_path = self.noisy_dir / f\"{basename}.wav\"\n",
    "\n",
    "        # # Load and resample clean audio to 16 kHz\n",
    "        clean_wav, sr = torchaudio.load(clean_path)\n",
    "        if sr != 16000:\n",
    "            clean_wav = torchaudio.functional.resample(clean_wav, sr, 16000)\n",
    "\n",
    "        # Load and resample noisy audio to 16 kHz\n",
    "        noisy_wav, sr = torchaudio.load(noisy_path)\n",
    "        if sr != 16000:\n",
    "            noisy_wav = torchaudio.functional.resample(noisy_wav, sr, 16000)\n",
    "\n",
    "        # --- To log-Mel spectrograms ---\n",
    "        clean_mel = torch.log1p(mel_spec_transform(clean_wav))\n",
    "        noisy_mel = torch.log1p(mel_spec_transform(noisy_wav))\n",
    "\n",
    "        # Normalize to roughly [-1,1]\n",
    "        clean_norm, clean_mn, clean_mx = normalize_minus_one_to_one(clean_mel)\n",
    "        noisy_norm, noisy_mn, noisy_mx = normalize_minus_one_to_one(noisy_mel)\n",
    "\n",
    "        freq_pad = 128 - clean_norm.size(1)  # clean_norm shape is [1,80,T]\n",
    "        if freq_pad > 0:\n",
    "            # pad dims = (time_left, time_right, freq_top, freq_bottom)\n",
    "            clean_norm = F.pad(clean_norm, (0, 0, 0, freq_pad))\n",
    "            noisy_norm = F.pad(noisy_norm, (0, 0, 0, freq_pad))\n",
    "\n",
    "        return (\n",
    "            noisy_norm, clean_norm,\n",
    "            (noisy_mn, noisy_mx, clean_mn, clean_mx),\n",
    "            basename\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds  = AudioDataset('./data', mode = 'train')\n",
    "\n",
    "sampler = BucketBatchSampler(\n",
    "    lengths    = train_ds.lengths,\n",
    "    batch_size = 32,\n",
    "    bucket_size= 32*10,   \n",
    ")\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_sampler = sampler,\n",
    "    collate_fn    = lambda b: pad_collate(b, T_max=512),\n",
    "    num_workers   = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_batch, clean_batch, stats, names = next(iter(train_dl))\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(10, 12))\n",
    "\n",
    "# for i in range(4):\n",
    "#     mn_no, mx_no, mn_cl, mx_cl = stats[i]\n",
    "\n",
    "#     noisy_log = (noisy_batch[i, 0] + 1) / 2 * (mx_no - mn_no) + mn_no\n",
    "#     clean_log = (clean_batch[i, 0] + 1) / 2 * (mx_cl - mn_cl) + mn_cl\n",
    "\n",
    "#     noisy_db = librosa.power_to_db(np.expm1(noisy_log.cpu().numpy()), ref=np.max)\n",
    "#     clean_db = librosa.power_to_db(np.expm1(clean_log.cpu().numpy()), ref=np.max)\n",
    "\n",
    "#     ax = axes[i, 0]\n",
    "#     librosa.display.specshow(\n",
    "#         noisy_db, sr=16000, hop_length=128,\n",
    "#         x_axis='time', y_axis='mel', ax=ax\n",
    "#     )\n",
    "#     ax.set_title(f\"Noisy ({names[i]})\")\n",
    "\n",
    "#     ax = axes[i, 1]\n",
    "#     librosa.display.specshow(\n",
    "#         clean_db, sr=16000, hop_length=128,\n",
    "#         x_axis='time', y_axis='mel', ax=ax\n",
    "#     )\n",
    "#     ax.set_title(f\"Clean ({names[i]})\")\n",
    "\n",
    "# fig.colorbar(axes[0,0].get_images()[0], ax=axes[:, :], format=\"%+2.f dB\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecPatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_features=64):\n",
    "        super().__init__()\n",
    "        # Since we concatenate two audios, the first conv sees in_channels*2\n",
    "        self.model = nn.Sequential(\n",
    "        # → (in_channels*2) x H x W\n",
    "        spectral_norm(nn.Conv2d(in_channels * 2, base_features, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → base_features x H/2 x W/2\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features, base_features*2, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*2) x H/4 x W/4\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features*2, base_features*4, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x H/8 x W/8\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features*4, base_features*8, kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x (H/8 - 1) x (W/8 - 1)\n",
    "\n",
    "        # final “patch” conv\n",
    "        spectral_norm(nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "        # → 1 x (H/8 - 2) x (W/8 - 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, spec_input, spec_target):\n",
    "        # spec_input and spec_target: [B, 1, H, W]\n",
    "        x = torch.cat([spec_input, spec_target], dim=1)  # → [B, 2, H, W]\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=64):\n",
    "        super().__init__()\n",
    "        # --- ENCODER (downsampling) ---\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H→H/2\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/2→H/4\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/4→H/8\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/8→H/16\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/16→H/32\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/32→H/64\n",
    "\n",
    "        # --- DECODER (upsampling) ---\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/256→H/128\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/128→H/64\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/64→H/32\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*12, features*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/32→H/16\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*6, features*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/16→H/8\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*3, features, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/8→H/4\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features, out_channels, kernel_size = 1, bias=False),\n",
    "        )                                   #  H/2→H\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.dec1(e6); d1 = torch.cat([d1, e5], dim=1)\n",
    "        d2 = self.dec2(d1); d2 = torch.cat([d2, e4], dim=1)\n",
    "        d3 = self.dec3(d2); d3 = torch.cat([d3, e3], dim=1)\n",
    "        d4 = self.dec4(d3); d4 = torch.cat([d4, e2], dim=1)\n",
    "        d5 = self.dec5(d4); d5 = torch.cat([d5, e1], dim=1)\n",
    "        x = self.dec6(d5)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, generator, noisy, clean, opt_d):\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(noisy, real) should predict “real” → target=1\n",
    "    real_preds = discriminator(noisy, clean)\n",
    "    real_targets = torch.full_like(real_preds, 0.9)\n",
    "    real_loss = F.binary_cross_entropy_with_logits(real_preds, real_targets)\n",
    "    real_score = real_preds.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(noisy) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_audios = generator(noisy).detach()\n",
    "    fake_preds = discriminator(noisy, fake_audios)\n",
    "    fake_targets = torch.zeros_like(fake_preds)\n",
    "    fake_loss    = F.binary_cross_entropy_with_logits(fake_preds, fake_targets)\n",
    "    fake_score   = fake_preds.mean().item() \n",
    "\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator, generator, noisy, clean, opt_g, lambda_L1 = 100):\n",
    "    generator.train()\n",
    "\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    # 1) Adverserial Loss\n",
    "    fake_audio = generator(noisy)\n",
    "\n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(noisy, fake_audio)\n",
    "    targets = torch.ones_like(preds)\n",
    "    adv_loss = F.binary_cross_entropy_with_logits(preds, targets)\n",
    "\n",
    "    # 2) L1 recontruction loss\n",
    "    l1_loss = F.l1_loss(fake_audio, clean)\n",
    "\n",
    "    total_loss = adv_loss + (lambda_L1 * l1_loss)\n",
    "\n",
    "    # Update generator weights\n",
    "    total_loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return total_loss.item(), adv_loss.item(), l1_loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize\n",
    "def denorm(normed, mn, mx):\n",
    "    \"\"\"\n",
    "    Inverts the above: takes a tensor in [-1,1] back to [mn,mx].\n",
    "    \"\"\"\n",
    "    return (normed + 1) / 2 * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio_samples(\n",
    "    index,\n",
    "    noisy_batch,\n",
    "    clean_batch,\n",
    "    generator,\n",
    "    denorm,\n",
    "    stats,              \n",
    "    sample_rate=16000,\n",
    "    sample_dir=\"audio_samples\",\n",
    "    show=True\n",
    "):\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    was_training = generator.training\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_batch = generator(noisy_batch.to(next(generator.parameters()).device))\n",
    "\n",
    "    if was_training:\n",
    "        generator.train()\n",
    "\n",
    "    for i, (noisy_mel, clean_mel, (nmn, nmx, cmn, cmx)) in enumerate(zip(noisy_batch, clean_batch, stats)):\n",
    "        fake_denorm = denorm(fake_batch[i], cmn, cmx)\n",
    "        lin_mel  = np.expm1(fake_denorm.cpu().numpy())\n",
    "        fake_wav = librosa.feature.inverse.mel_to_audio(\n",
    "            lin_mel, sr=sample_rate, hop_length=128, n_fft=512, n_iter=32\n",
    "        )\n",
    "\n",
    "        prefix = f\"{index:04d}_{i}\"\n",
    "        sf.write(os.path.join(sample_dir, f\"{prefix}_denoised.wav\"), fake_wav, sample_rate)\n",
    "\n",
    "        if show:\n",
    "            db = librosa.power_to_db(lin_mel, ref=np.max)\n",
    "            plt.figure(figsize=(6,2))\n",
    "            librosa.display.specshow(db, sr=sample_rate, hop_length=128, y_axis=\"mel\", x_axis=\"time\")\n",
    "            plt.title(f\"Denoised ({prefix})\")\n",
    "            plt.colorbar(format=\"%+2.0f dB\")\n",
    "            plt.show()\n",
    "\n",
    "    print(f\"Saved audio samples for batch index {index} → `{sample_dir}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Loop with ASR-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    discriminator: nn.Module,\n",
    "    generator:     nn.Module,\n",
    "    train_dl,\n",
    "    fixed_noisy,  \n",
    "    fixed_clean, \n",
    "    fixed_stats,        \n",
    "    denorm,                \n",
    "    device,\n",
    "    epochs     = 200,\n",
    "    lr         = 2e-4,\n",
    "    lambda_L1  = 100,\n",
    "    start_idx  = 1\n",
    "):\n",
    "\n",
    "    # Optimizers\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(),     lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # History\n",
    "    losses_d, losses_g = [], []\n",
    "    real_scores, fake_scores = [], []\n",
    "\n",
    "    for epoch in range(start_idx, start_idx + epochs):\n",
    "        sum_d = sum_g = 0.0\n",
    "        sum_real = sum_fake = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{start_idx+epochs-1}\")\n",
    "        for noisy, clean, stats, names in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            # ——— Train D ———\n",
    "            opt_d.zero_grad()\n",
    "            d_loss, real_s, fake_s = train_discriminator(\n",
    "                discriminator, generator,\n",
    "                noisy, clean,\n",
    "                opt_d\n",
    "            )\n",
    "\n",
    "            # ——— Train G ———\n",
    "            opt_g.zero_grad()\n",
    "            g_loss, adv_loss, l1_loss = train_generator(\n",
    "                discriminator, generator,\n",
    "                noisy, clean,\n",
    "                opt_g,\n",
    "                lambda_L1\n",
    "            )\n",
    "\n",
    "            # Accumulate stats\n",
    "            sum_d    += d_loss\n",
    "            sum_g    += g_loss\n",
    "            sum_real += real_s\n",
    "            sum_fake += fake_s\n",
    "            batches  += 1\n",
    "\n",
    "        # Compute averages\n",
    "        avg_d    = sum_d    / batches\n",
    "        avg_g    = sum_g    / batches\n",
    "        avg_real = sum_real / batches\n",
    "        avg_fake = sum_fake / batches\n",
    "\n",
    "        # Record losses & scores\n",
    "        losses_d.append(avg_d)\n",
    "        losses_g.append(avg_g)\n",
    "        real_scores.append(avg_real)\n",
    "        fake_scores.append(avg_fake)\n",
    "\n",
    "        # Log losses & scores\n",
    "        print(\n",
    "            f\"Epoch [{epoch}]  \"\n",
    "            f\"loss_g: {avg_g:.4f}, loss_d: {avg_d:.4f}, \"\n",
    "            f\"real_score: {avg_real:.4f}, fake_score: {avg_fake:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Generate & Save fixed-noisy samples\n",
    "        save_audio_samples(\n",
    "            index       = epoch,\n",
    "            noisy_batch = fixed_noisy.to(device),\n",
    "            clean_batch = fixed_clean.to(device),\n",
    "            generator   = generator,\n",
    "            denorm      = denorm,\n",
    "            stats       = fixed_stats,\n",
    "            show        = False\n",
    "            )\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(generator.state_dict(), f\"checkpoint_gen_epoch{epoch}.pth\")\n",
    "\n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight-initialisation helper\n",
    "def init_weights(m):\n",
    "    \"\"\"DCGAN‐style weight init: N(0, 0.02) for Conv / BN layers.\"\"\"\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = SpecUNetGenerator()\n",
    "discriminator = SpecPatchDiscriminator()\n",
    "\n",
    "generator.apply(init_weights) \n",
    "discriminator.apply(init_weights) \n",
    "\n",
    "discriminator = discriminator.to(device)\n",
    "generator     = generator.to(device)\n",
    "\n",
    "fixed_noisy, fixed_clean, fixed_stats, _= next(iter(train_dl))\n",
    "\n",
    "fixed_noisy = fixed_noisy.to(device)\n",
    "fixed_clean = fixed_clean.to(device)\n",
    "\n",
    "history = fit(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    train_dl=train_dl,\n",
    "    fixed_noisy=fixed_noisy,\n",
    "    fixed_clean=fixed_clean,\n",
    "    fixed_stats=fixed_stats,\n",
    "    denorm=denorm,\n",
    "    device=device,    \n",
    "    epochs=200,\n",
    "    lr=2e-4,\n",
    "    lambda_L1=100,\n",
    "    start_idx=1\n",
    ")\n",
    "\n",
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator.state_dict(), 'G.pth')\n",
    "torch.save(discriminator.state_dict(), 'D.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "epochs = [1, 5, 10, 50, 100, 150, 200]\n",
    "\n",
    "for e in epochs:\n",
    "    tag = f\"{e:04d}_0\"\n",
    "    print(f\"Epoch {e}:\")\n",
    "    display(Audio(f\"audio_samples/{tag}_denoised.wav\", rate=16000))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = list(range(1, len(losses_d) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_d, label=\"Discriminator\")\n",
    "plt.plot(epochs_range, losses_g, label=\"Generator\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Real & Fake Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, real_scores, label=\"Real Score\")\n",
    "plt.plot(epochs_range, fake_scores, label=\"Fake Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Real vs Fake Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_audio(audio_path):\n",
    "    # ensure the generator is in eval mode for inference\n",
    "    generator.eval()\n",
    "\n",
    "    wav, sr = librosa.load(audio_path, sr=16000)\n",
    "    wav_t = torch.from_numpy(wav).unsqueeze(0).to(device)\n",
    "    mel   = torch.log1p(mel_spec_transform(wav_t))          # [1,80,T']\n",
    "    mel_norm, mn, mx = normalize_minus_one_to_one(mel)      # [1,80,T']\n",
    "\n",
    "    # Pad frequency axis to 128 bins\n",
    "    freq_pad = 128 - mel_norm.size(1)\n",
    "    if freq_pad > 0:\n",
    "        mel_norm = F.pad(mel_norm, (0, 0, 0, freq_pad))     # [1,128,T']\n",
    "\n",
    "    # Truncate/pad to T_max=512\n",
    "    mel_norm = mel_norm[..., :512]\n",
    "    if mel_norm.shape[-1] < 512:\n",
    "        pad = 512 - mel_norm.shape[-1]\n",
    "        mel_norm = F.pad(mel_norm, (0, pad))\n",
    "\n",
    "    mel_in = mel_norm.unsqueeze(0)                         # [1,1,128,512]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = generator(mel_in)                           # [1,1,128,512]\n",
    "\n",
    "    fake = fake.squeeze(0).squeeze(0).cpu()                # [128,512]\n",
    "    fake = denorm(fake, mn, mx)                            # back to log-Mel\n",
    "\n",
    "    lin_mel = np.expm1(fake.numpy())\n",
    "    denoised = librosa.feature.inverse.mel_to_audio(\n",
    "        lin_mel, sr=sr, hop_length=128, n_fft=512, n_iter=32\n",
    "    )\n",
    "\n",
    "    return sr, denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=enhance_audio,\n",
    "    inputs=gr.Audio(source=\"upload\", type=\"filepath\", label=\"Noisy Audio\"),\n",
    "    outputs=gr.Audio(type=\"numpy\", label=\"Denoised Audio\"),\n",
    "    title=\"Speech Enhancement GAN\",\n",
    "    description=\"Upload a WAV file sampled at 16 kHz and get back the denoised audio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
