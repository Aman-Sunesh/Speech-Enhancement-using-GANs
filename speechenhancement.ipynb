{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Enhancement using Conditional GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NYUAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System & Utilities\n",
    "import os\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Audio I/O & Processing\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# PyTorch Model Building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "# Demo / Deployment\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the mel spectrogram transform\n",
    "mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate = 16000,\n",
    "                                                         n_fft = 512, \n",
    "                                                         hop_length = 128, \n",
    "                                                         n_mels = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minus_one_to_one(data):\n",
    "    x_min = data.min()\n",
    "    x_max = data.max()\n",
    "    normalized_data = 2 * ((data - x_min) / (x_max - x_min)) - 1\n",
    "    return normalized_data, x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size, bucket_size = 1000, shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        sorted_indices = []\n",
    "        buckets = []\n",
    "\n",
    "        # Build list of (length, index) pairs\n",
    "        for i in range(len(lengths)):\n",
    "            sorted_indices.append((lengths[i], i))\n",
    "\n",
    "        # Sort by length\n",
    "        sorted_indices.sort()\n",
    "\n",
    "        # Keep only the indices in order\n",
    "        for j in range(len(sorted_indices)):\n",
    "            sorted_indices[j] = sorted_indices[j][1]\n",
    "\n",
    "        # Break into buckets of size 'bucket_size'\n",
    "        for i in range(0, len(sorted_indices), bucket_size):\n",
    "            bucket = sorted_indices[i : i + bucket_size]\n",
    "            buckets.append(bucket)\n",
    "\n",
    "        self.buckets = buckets\n",
    "\n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            if self.shuffle:\n",
    "                random.shuffle(bucket)\n",
    "\n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                yield bucket[i : i + self.batch_size]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        sum_batches = 0\n",
    "\n",
    "        for b in self.buckets:\n",
    "            sum_batches += (len(b) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        return sum_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch, T_max=512):\n",
    "    noisys, cleans, stats, names = [], [], [], []\n",
    "\n",
    "    for noisy, clean, st, name in batch:\n",
    "        # Truncate\n",
    "        noisy = noisy[..., :T_max]\n",
    "        clean = clean[..., :T_max]\n",
    "\n",
    "        # Pad if shorter\n",
    "        pad_no = T_max - noisy.shape[-1]\n",
    "        pad_cl = T_max - clean.shape[-1]\n",
    "\n",
    "        if pad_no>0:\n",
    "            noisy = F.pad(noisy, (0, pad_no))\n",
    "            clean = F.pad(clean, (0, pad_cl))\n",
    "\n",
    "        noisys.append(noisy)\n",
    "        cleans.append(clean)\n",
    "\n",
    "        stats.append(st)\n",
    "        names.append(name)\n",
    "\n",
    "    return torch.stack(noisys), torch.stack(cleans), stats, names\n",
    "\n",
    "def my_collate(batch):\n",
    "    return pad_collate(batch, T_max=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root, max_samples=None, max_len=512):\n",
    "        root = Path(root)\n",
    "        self.clean_files = sorted((root/\"clean_trainset_wav\").glob(\"*.wav\"))\n",
    "        self.noisy_files = sorted((root/\"noisy_trainset_wav\").glob(\"*.wav\"))\n",
    "        assert len(self.clean_files)==len(self.noisy_files)\n",
    "        if max_samples:\n",
    "            self.clean_files = self.clean_files[:max_samples]\n",
    "            self.noisy_files = self.noisy_files[:max_samples]\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.sr      = 16000\n",
    "        self.hop     = 128\n",
    "        self.mel     = mel_spec_transform\n",
    "\n",
    "        # precompute mel-frame lengths\n",
    "        self.lengths = []\n",
    "        for p in self.clean_files:\n",
    "            info = torchaudio.info(p)\n",
    "            n_frames = info.num_frames\n",
    "            if info.sample_rate!=self.sr:\n",
    "                n_frames = int(n_frames*(self.sr/info.sample_rate))\n",
    "            self.lengths.append(ceil(n_frames/self.hop))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_path = self.clean_files[idx]\n",
    "        noisy_path = self.noisy_files[idx]\n",
    "\n",
    "        # load + resample\n",
    "        wav_cl, sr0 = torchaudio.load(clean_path)\n",
    "        wav_no, sr1 = torchaudio.load(noisy_path)\n",
    "        if sr0!=self.sr:\n",
    "            wav_cl = torchaudio.functional.resample(wav_cl, sr0, self.sr)\n",
    "        if sr1!=self.sr:\n",
    "            wav_no = torchaudio.functional.resample(wav_no, sr1, self.sr)\n",
    "\n",
    "        # to log-mel\n",
    "        mel_cl = torch.log1p(self.mel(wav_cl))\n",
    "        mel_no = torch.log1p(self.mel(wav_no))\n",
    "\n",
    "        # normalize\n",
    "        norm_cl, mn_cl, mx_cl = normalize_minus_one_to_one(mel_cl)\n",
    "        norm_no, mn_no, mx_no = normalize_minus_one_to_one(mel_no)\n",
    "\n",
    "        # pad freq →128 bins\n",
    "        freq_pad = 128 - norm_cl.size(1)\n",
    "        if freq_pad>0:\n",
    "            norm_cl = F.pad(norm_cl, (0,0,freq_pad,0))\n",
    "            norm_no = F.pad(norm_no, (0,0,freq_pad,0))\n",
    "\n",
    "        # pad/truncate time to max_len\n",
    "        if norm_cl.size(-1)<self.max_len:\n",
    "            p = self.max_len - norm_cl.size(-1)\n",
    "            norm_cl = F.pad(norm_cl, (0,p))\n",
    "            norm_no = F.pad(norm_no, (0,p))\n",
    "        else:\n",
    "            norm_cl = norm_cl[..., :self.max_len]\n",
    "            norm_no = norm_no[..., :self.max_len]\n",
    "\n",
    "        stats = (mn_no, mx_no, mn_cl, mx_cl)\n",
    "        name  = clean_path.stem\n",
    "        return norm_no.unsqueeze(0), norm_cl.unsqueeze(0), stats, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "train_ds = AudioDataset(\"./data\", max_samples=N)\n",
    "sampler  = BucketBatchSampler(train_ds.lengths, batch_size=4, bucket_size=4*3)\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_sampler=sampler,\n",
    "                      collate_fn=my_collate,\n",
    "                      num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_batch, clean_batch, stats, names = next(iter(train_dl))\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(10, 12))\n",
    "\n",
    "# for i in range(4):\n",
    "#     mn_no, mx_no, mn_cl, mx_cl = stats[i]\n",
    "\n",
    "#     noisy_log = (noisy_batch[i, 0] + 1) / 2 * (mx_no - mn_no) + mn_no\n",
    "#     clean_log = (clean_batch[i, 0] + 1) / 2 * (mx_cl - mn_cl) + mn_cl\n",
    "\n",
    "#     noisy_db = librosa.power_to_db(np.expm1(noisy_log.cpu().numpy()), ref=np.max)\n",
    "#     clean_db = librosa.power_to_db(np.expm1(clean_log.cpu().numpy()), ref=np.max)\n",
    "\n",
    "#     ax = axes[i, 0]\n",
    "#     librosa.display.specshow(\n",
    "#         noisy_db, sr=16000, hop_length=128,\n",
    "#         x_axis='time', y_axis='mel', ax=ax\n",
    "#     )\n",
    "#     ax.set_title(f\"Noisy ({names[i]})\")\n",
    "\n",
    "#     ax = axes[i, 1]\n",
    "#     librosa.display.specshow(\n",
    "#         clean_db, sr=16000, hop_length=128,\n",
    "#         x_axis='time', y_axis='mel', ax=ax\n",
    "#     )\n",
    "#     ax.set_title(f\"Clean ({names[i]})\")\n",
    "\n",
    "# fig.colorbar(axes[0,0].get_images()[0], ax=axes[:, :], format=\"%+2.f dB\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecPatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_features=64):\n",
    "        super().__init__()\n",
    "        # Since we concatenate two audios, the first conv sees in_channels*2\n",
    "        self.model = nn.Sequential(\n",
    "        # → (in_channels*2) x H x W\n",
    "        spectral_norm(nn.Conv2d(in_channels * 2, base_features, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → base_features x H/2 x W/2\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features, base_features*2, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*2) x H/4 x W/4\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features*2, base_features*4, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x H/8 x W/8\n",
    "\n",
    "        spectral_norm(nn.Conv2d(base_features*4, base_features*8, kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "        nn.BatchNorm2d(base_features*8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x (H/8 - 1) x (W/8 - 1)\n",
    "\n",
    "        # final “patch” conv\n",
    "        spectral_norm(nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "        # → 1 x (H/8 - 2) x (W/8 - 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, spec_input, spec_target):\n",
    "        # spec_input and spec_target: [B, 1, H, W]\n",
    "        x = torch.cat([spec_input, spec_target], dim=1)  # → [B, 2, H, W]\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=64):\n",
    "        super().__init__()\n",
    "        # --- ENCODER (downsampling) ---\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H→H/2\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/2→H/4\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/4→H/8\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/8→H/16\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/16→H/32\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )                                   #  H/32→H/64\n",
    "\n",
    "        # --- DECODER (upsampling) ---\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/256→H/128\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/128→H/64\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/64→H/32\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*12, features*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/32→H/16\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*6, features*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/16→H/8\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*3, features, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                   #  H/8→H/4\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features, out_channels, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )                               #  H/2→H\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.dec1(e6); d1 = torch.cat([d1, e5], dim=1)\n",
    "        d2 = self.dec2(d1); d2 = torch.cat([d2, e4], dim=1)\n",
    "        d3 = self.dec3(d2); d3 = torch.cat([d3, e3], dim=1)\n",
    "        d4 = self.dec4(d3); d4 = torch.cat([d4, e2], dim=1)\n",
    "        d5 = self.dec5(d4); d5 = torch.cat([d5, e1], dim=1)\n",
    "        x = self.dec6(d5)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, generator, noisy, clean, opt_d):\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(noisy, real) should predict “real” → target=1\n",
    "    real_preds = discriminator(noisy, clean)\n",
    "    real_targets = torch.full_like(real_preds, 0.9)\n",
    "    real_loss = F.binary_cross_entropy_with_logits(real_preds, real_targets)\n",
    "    real_score = real_preds.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(noisy) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_audios = generator(noisy).detach()\n",
    "    fake_preds = discriminator(noisy, fake_audios)\n",
    "    fake_targets = torch.zeros_like(fake_preds)\n",
    "    fake_loss    = F.binary_cross_entropy_with_logits(fake_preds, fake_targets)\n",
    "    fake_score   = fake_preds.mean().item() \n",
    "\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator, generator, noisy, clean, opt_g, lambda_L1 = 100):\n",
    "    generator.train()\n",
    "\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    # 1) Adverserial Loss\n",
    "    fake_audio = generator(noisy)\n",
    "\n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(noisy, fake_audio)\n",
    "    targets = torch.ones_like(preds)\n",
    "    adv_loss = F.binary_cross_entropy_with_logits(preds, targets)\n",
    "\n",
    "    # 2) L1 recontruction loss\n",
    "    l1_loss = F.l1_loss(fake_audio, clean)\n",
    "\n",
    "    total_loss = adv_loss + (lambda_L1 * l1_loss)\n",
    "\n",
    "    # Update generator weights\n",
    "    total_loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return total_loss.item(), adv_loss.item(), l1_loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize\n",
    "def denorm(normed, mn, mx):\n",
    "    \"\"\"\n",
    "    Inverts the above: takes a tensor in [-1,1] back to [mn,mx].\n",
    "    \"\"\"\n",
    "    return (normed + 1) / 2 * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio_samples(\n",
    "    index,\n",
    "    noisy_batch,\n",
    "    clean_batch,\n",
    "    generator,\n",
    "    denorm,\n",
    "    stats,              \n",
    "    sample_rate=16000,\n",
    "    sample_dir=\"audio_samples\",\n",
    "    show=True\n",
    "):\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    was_training = generator.training\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_batch = generator(noisy_batch.to(next(generator.parameters()).device))\n",
    "\n",
    "    if was_training:\n",
    "        generator.train()\n",
    "\n",
    "    for i, (noisy_mel, clean_mel, (nmn, nmx, cmn, cmx)) in enumerate(zip(noisy_batch, clean_batch, stats)):\n",
    "        fake_denorm = denorm(fake_batch[i], cmn, cmx)\n",
    "        lin_mel  = np.expm1(fake_denorm.cpu().numpy())\n",
    "        fake_wav = librosa.feature.inverse.mel_to_audio(\n",
    "            lin_mel, sr=sample_rate, hop_length=128, n_fft=512, n_iter=32\n",
    "        )\n",
    "\n",
    "        prefix = f\"{index:04d}_{i}\"\n",
    "        sf.write(os.path.join(sample_dir, f\"{prefix}_denoised.wav\"), fake_wav, sample_rate)\n",
    "\n",
    "        if show:\n",
    "            db = librosa.power_to_db(lin_mel, ref=np.max)\n",
    "            plt.figure(figsize=(6,2))\n",
    "            librosa.display.specshow(db, sr=sample_rate, hop_length=128, y_axis=\"mel\", x_axis=\"time\")\n",
    "            plt.title(f\"Denoised ({prefix})\")\n",
    "            plt.colorbar(format=\"%+2.0f dB\")\n",
    "            plt.show()\n",
    "\n",
    "    print(f\"Saved audio samples for batch index {index} → `{sample_dir}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Loop with ASR-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    discriminator: nn.Module,\n",
    "    generator:     nn.Module,\n",
    "    train_dl,\n",
    "    fixed_noisy,  \n",
    "    fixed_clean, \n",
    "    fixed_stats,        \n",
    "    denorm,                \n",
    "    device,\n",
    "    epochs     = 200,\n",
    "    lr         = 2e-4,\n",
    "    lambda_L1  = 100,\n",
    "    start_idx  = 1\n",
    "):\n",
    "\n",
    "    # Optimizers\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(),     lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # History\n",
    "    losses_d, losses_g = [], []\n",
    "    real_scores, fake_scores = [], []\n",
    "\n",
    "    for epoch in range(start_idx, start_idx + epochs):\n",
    "        sum_d = sum_g = 0.0\n",
    "        sum_real = sum_fake = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{start_idx+epochs-1}\")\n",
    "        for noisy, clean, stats, names in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            # ——— Train D ———\n",
    "            opt_d.zero_grad()\n",
    "            d_loss, real_s, fake_s = train_discriminator(\n",
    "                discriminator, generator,\n",
    "                noisy, clean,\n",
    "                opt_d\n",
    "            )\n",
    "\n",
    "            # ——— Train G ———\n",
    "            opt_g.zero_grad()\n",
    "            g_loss, adv_loss, l1_loss = train_generator(\n",
    "                discriminator, generator,\n",
    "                noisy, clean,\n",
    "                opt_g,\n",
    "                lambda_L1\n",
    "            )\n",
    "\n",
    "            # Accumulate stats\n",
    "            sum_d    += d_loss\n",
    "            sum_g    += g_loss\n",
    "            sum_real += real_s\n",
    "            sum_fake += fake_s\n",
    "            batches  += 1\n",
    "\n",
    "        # Compute averages\n",
    "        avg_d    = sum_d    / batches\n",
    "        avg_g    = sum_g    / batches\n",
    "        avg_real = sum_real / batches\n",
    "        avg_fake = sum_fake / batches\n",
    "\n",
    "        # Record losses & scores\n",
    "        losses_d.append(avg_d)\n",
    "        losses_g.append(avg_g)\n",
    "        real_scores.append(avg_real)\n",
    "        fake_scores.append(avg_fake)\n",
    "\n",
    "        # Log losses & scores\n",
    "        print(\n",
    "            f\"Epoch [{epoch}]  \"\n",
    "            f\"loss_g: {avg_g:.4f}, loss_d: {avg_d:.4f}, \"\n",
    "            f\"real_score: {avg_real:.4f}, fake_score: {avg_fake:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Generate & Save fixed-noisy samples\n",
    "        save_audio_samples(\n",
    "            index       = epoch,\n",
    "            noisy_batch = fixed_noisy.to(device),\n",
    "            clean_batch = fixed_clean.to(device),\n",
    "            generator   = generator,\n",
    "            denorm      = denorm,\n",
    "            stats       = fixed_stats,\n",
    "            show        = False\n",
    "            )\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(generator.state_dict(), f\"checkpoint_gen_epoch{epoch}.pth\")\n",
    "\n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight-initialisation helper\n",
    "def init_weights(m):\n",
    "    \"\"\"DCGAN‐style weight init: N(0, 0.02) for Conv / BN layers.\"\"\"\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioDataset' object has no attribute 'file_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m discriminator = discriminator.to(device)\n\u001b[32m     10\u001b[39m generator     = generator.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m fixed_noisy, fixed_clean, fixed_stats, _= \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m fixed_noisy = fixed_noisy.to(device)\n\u001b[32m     15\u001b[39m fixed_clean = fixed_clean.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mAudioDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     stem = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_list\u001b[49m[idx]\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# 1) load clean & noisy waveforms at 16 kHz\u001b[39;00m\n\u001b[32m     28\u001b[39m     wav_no = torchaudio.load(\u001b[38;5;28mself\u001b[39m.noisy_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.wav\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'AudioDataset' object has no attribute 'file_list'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = SpecUNetGenerator()\n",
    "discriminator = SpecPatchDiscriminator()\n",
    "\n",
    "generator.apply(init_weights) \n",
    "discriminator.apply(init_weights) \n",
    "\n",
    "discriminator = discriminator.to(device)\n",
    "generator     = generator.to(device)\n",
    "\n",
    "fixed_noisy, fixed_clean, fixed_stats, _= next(iter(train_dl))\n",
    "\n",
    "fixed_noisy = fixed_noisy.to(device)\n",
    "fixed_clean = fixed_clean.to(device)\n",
    "\n",
    "history = fit(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    train_dl=train_dl,\n",
    "    fixed_noisy=fixed_noisy,\n",
    "    fixed_clean=fixed_clean,\n",
    "    fixed_stats=fixed_stats,\n",
    "    denorm=denorm,\n",
    "    device=device,    \n",
    "    epochs=2,\n",
    "    lr=2e-4,\n",
    "    lambda_L1=100,\n",
    "    start_idx=1\n",
    ")\n",
    "\n",
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator.state_dict(), 'G.pth')\n",
    "torch.save(discriminator.state_dict(), 'D.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "epochs = [1, 5, 10, 50, 100, 150, 200]\n",
    "\n",
    "for e in epochs:\n",
    "    tag = f\"{e:04d}_0\"\n",
    "    print(f\"Epoch {e}:\")\n",
    "    display(Audio(f\"audio_samples/{tag}_denoised.wav\", rate=16000))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = list(range(1, len(losses_d) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_d, label=\"Discriminator\")\n",
    "plt.plot(epochs_range, losses_g, label=\"Generator\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Real & Fake Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, real_scores, label=\"Real Score\")\n",
    "plt.plot(epochs_range, fake_scores, label=\"Fake Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Real vs Fake Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_audio(audio_path):\n",
    "    # ensure the generator is in eval mode for inference\n",
    "    generator.eval()\n",
    "\n",
    "    wav, sr = librosa.load(audio_path, sr=16000)\n",
    "    wav_t = torch.from_numpy(wav).unsqueeze(0).to(device)\n",
    "    mel   = torch.log1p(mel_spec_transform(wav_t))          # [1,80,T']\n",
    "    mel_norm, mn, mx = normalize_minus_one_to_one(mel)      # [1,80,T']\n",
    "\n",
    "    # Pad frequency axis to 128 bins\n",
    "    freq_pad = 128 - mel_norm.size(1)\n",
    "    if freq_pad > 0:\n",
    "        mel_norm = F.pad(mel_norm, (0, 0, freq_pad, 0))     # [1,128,T']\n",
    "\n",
    "    # Truncate/pad to T_max=512\n",
    "    mel_norm = mel_norm[..., :512]\n",
    "    if mel_norm.shape[-1] < 512:\n",
    "        pad = 512 - mel_norm.shape[-1]\n",
    "        mel_norm = F.pad(mel_norm, (0, pad))\n",
    "\n",
    "    mel_in = mel_norm.unsqueeze(0)                         # [1,1,128,512]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = generator(mel_in)                           # [1,1,128,512]\n",
    "\n",
    "    fake = fake.squeeze(0).squeeze(0).cpu()                # [128,512]\n",
    "    fake = denorm(fake, mn, mx)                            # back to log-Mel\n",
    "\n",
    "    lin_mel = np.expm1(fake.numpy())\n",
    "    denoised = librosa.feature.inverse.mel_to_audio(\n",
    "        lin_mel, sr=sr, hop_length=128, n_fft=512, n_iter=32\n",
    "    )\n",
    "\n",
    "    return sr, denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=enhance_audio,\n",
    "    inputs=gr.Audio(source=\"upload\", type=\"filepath\", label=\"Noisy Audio\"),\n",
    "    outputs=gr.Audio(type=\"numpy\", label=\"Denoised Audio\"),\n",
    "    title=\"Speech Enhancement GAN\",\n",
    "    description=\"Upload a WAV file sampled at 16 kHz and get back the denoised audio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
